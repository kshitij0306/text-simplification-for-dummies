# Text Simplification with Curriculum Learning

## Overview
This project, developed at the University of Massachusetts Lowell, pioneers an innovative approach in text simplification. Utilizing advanced techniques like BERT-based encoder-decoder models, curriculum learning, and recall strategies, our method significantly improves readability while retaining the content's integrity. This is particularly beneficial for non-native speakers and individuals with reading challenges.

## Features
- **BERT-based Encoder-Decoder Model**: Leverages deep learning for nuanced text interpretation and simplification.
- **Curriculum Learning**: Sequential training on texts with varying difficulty levels, enhancing the model's adaptability.
- **Recall Strategies**: Integrating psychology of learning for efficient text processing.
- **Diverse Text Structures**: Ability to handle a range of complexities from academic to casual language.

## Applications
- Educational tools for language learning.
- Accessibility software for reading assistance.
- Content creation and simplification for broader audience reach.

## Performance
The model has shown promising results in terms of readability and complexity management, as evaluated using SARI scores. This underscores its efficacy in text simplification while maintaining semantic fidelity.

## Installation
To install the required dependencies, run the following command in your terminal:
```bash
pip install -r requirements.txt
```
This will install all necessary libraries, including BERT-based models, NLP tools, and various Python packages needed for the project.

## Usage
To use the text simplification model, run:
```bash
python run.py
```
This script will initiate the text simplification process. For advanced options, such as curriculum learning and recall strategies, refer to the scheduler.py and tokenizer.py scripts.

## Contributing
Contributions are welcome! If you'd like to contribute, please fork the repository and use a feature branch. Pull requests are warmly welcomed.
- **Coding Standards**: Please ensure your code adheres to PEP-8 standards.
- **Pull Requests**: Describe what the PR solves or improves. Include relevant issue numbers if applicable.

## License
This project is licensed under the MIT License. See the LICENSE file for more details.

## Keywords
- Text Simplification
- BERT Encoder-Decoder
- Curriculum Learning
- Large Language Models (LLM)
- Hugging Face
- Natural Language Processing (NLP)
- Readability Improvement
- Machine Learning
- AI in Education
- Semantic Integrity
